{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"computation wiki page for CIDC \u00b6","title":"Home"},{"location":"#computation-wiki-page-for-cidc","text":"","title":"computation wiki page for CIDC"},{"location":"google-cloud/chips-gcp/","text":"Thanks Len for writing the instructions! clone the chips automator repository: \u00b6 git clone git@bitbucket.org:plumbers/chips_automator.git #NOTE: you only need to do this step once build the chips_automator conda environment and activate it: \u00b6 cd chips_automator conda env create -f chips_automator_env.yml source activate chips_automator #NOTE: you only need to do this step once Authenticate/get copy of google cloud key: \u00b6 gcloud auth application-default login #And follow the directions #NOTE: if successful, there should be a new file called ~/.ssh/google_cloud_engine #NOTE: you only need to do this step once By default, it is authenticated using Cloud SDK. To follow this, has to use a ssh authentication . Go to https://source.cloud.google.com/user/ssh_keys and copy your public key ~/.ssh/id_rsa.pub to register the key. Create a chips_automator run configuration file: \u00b6 a. copy over template: NOTE: in this example we're going to name our chips_automator conf file test.config.yaml b. Edit test.config.yaml: instance_name: define the instance name- any arbitrary string but cannon contain '.' cores: define the number of cores for the instance--default is 32 disk_size: define the size of the attached disk google_bucket_path: define the google bucket path to store the results when run is complete (optional) chips_commit: define the exact chips commit version to use samples: define the sample names and the google bucket paths to their fastqs meta: group samples into Treat/Control (i.e. input) Run chips automator: \u00b6 ./chips_automator.py -c test.config.yaml -u [ your google cloud username--usually your hostname ] -k ~/.ssh/google_cloud_enging Chips automator should run successfully, it will print diagnostic messages until it finishes. If you encounter an error, please send the output of chips automator to Len. The last output should be the ip addr of the instance. Log into the instance to check on the run: \u00b6 ssh -i ~/.ssh/google_cloud_engine [ username ] @ [ ip addr from last line of chips_automator ] cd /mnt/ssd/chips check the state of the chips run by looking at /mnt/ssd/chips/nohup.out When the run is complete or stops at an error state, don't forget to delete the instance.","title":"Run CHIPS on gcp"},{"location":"google-cloud/chips-gcp/#clone-the-chips-automator-repository","text":"git clone git@bitbucket.org:plumbers/chips_automator.git #NOTE: you only need to do this step once","title":"clone the chips automator repository:"},{"location":"google-cloud/chips-gcp/#build-the-chips_automator-conda-environment-and-activate-it","text":"cd chips_automator conda env create -f chips_automator_env.yml source activate chips_automator #NOTE: you only need to do this step once","title":"build the chips_automator conda environment and activate it:"},{"location":"google-cloud/chips-gcp/#authenticateget-copy-of-google-cloud-key","text":"gcloud auth application-default login #And follow the directions #NOTE: if successful, there should be a new file called ~/.ssh/google_cloud_engine #NOTE: you only need to do this step once By default, it is authenticated using Cloud SDK. To follow this, has to use a ssh authentication . Go to https://source.cloud.google.com/user/ssh_keys and copy your public key ~/.ssh/id_rsa.pub to register the key.","title":"Authenticate/get copy of google cloud key:"},{"location":"google-cloud/chips-gcp/#create-a-chips_automator-run-configuration-file","text":"a. copy over template: NOTE: in this example we're going to name our chips_automator conf file test.config.yaml b. Edit test.config.yaml: instance_name: define the instance name- any arbitrary string but cannon contain '.' cores: define the number of cores for the instance--default is 32 disk_size: define the size of the attached disk google_bucket_path: define the google bucket path to store the results when run is complete (optional) chips_commit: define the exact chips commit version to use samples: define the sample names and the google bucket paths to their fastqs meta: group samples into Treat/Control (i.e. input)","title":"Create a chips_automator run configuration file:"},{"location":"google-cloud/chips-gcp/#run-chips-automator","text":"./chips_automator.py -c test.config.yaml -u [ your google cloud username--usually your hostname ] -k ~/.ssh/google_cloud_enging Chips automator should run successfully, it will print diagnostic messages until it finishes. If you encounter an error, please send the output of chips automator to Len. The last output should be the ip addr of the instance.","title":"Run chips automator:"},{"location":"google-cloud/chips-gcp/#log-into-the-instance-to-check-on-the-run","text":"ssh -i ~/.ssh/google_cloud_engine [ username ] @ [ ip addr from last line of chips_automator ] cd /mnt/ssd/chips check the state of the chips run by looking at /mnt/ssd/chips/nohup.out When the run is complete or stops at an error state, don't forget to delete the instance.","title":"Log into the instance to check on the run:"},{"location":"google-cloud/wes-gcp/","text":"How to run WES pipeline in google cloud \u00b6 Thanks Len and Aashna for sharing it. how to get your google cloud credentials \u00b6 a. install the gcloud sdk: (it's on conda) conda install -c conda-forge google-cloud-sdk b. Authenticate: gcloud auth login NOTE: opens up a webbrowser, where I need to select the google account to allow access c. set project: gcloud config set project cidc-biofx CREATE the new instance: \u00b6 a. Goto google cloud platform : Google Compute Engine -> click \"Create Instance\" give your instance type a unique name, e.g. wes-aashna-1 Machine type: select high-mem-64 (this has 64 cores) Boot disk: click \"Change\". click \"Custome Images\" (next to Application Images). select latest wes image-. AS OF 2019-02-04, it is wes-ver-1-1b Click \"Management, security, disks, networking, sole tenancy\". click Disks click \"Add new disk\" scroll down to size, and HERE you need to try to predict how much space you need to do your analysis e.g. 2T = 2048 scroll to the bottom, click \"Create\" NOW you will be brough back to the Google Compute Engine page click on your new instance when it is up and get the IP address login to your instance: gcloud compute ssh test-instance-1 WHERE test-instance-1 is the instance name. NOTE: this creates ~/.ssh/google_compute_engine and ~/.ssh/google_compute._engine.pub **NOTE: You only need to do this once. after ~/.ssh/google_compute_engine and ~/.ssh/google_compute._engine.pub is created, you can use this ssh cmd to log in instead: ssh -i [ your google-cloud-engine key ] [ your username ] @ [ ipaddress in 1b. ] e.g. ssh -i ~/.ssh/google_compute_engine aashna@XX.YY.ZZ.AA Please read Connecting to Linux instances and Connecting to instances using advanced methods for more details. formatting and mounting the second drive- \u00b6 After logging in, the next step is to mount and format the drive that you apportioned in step 1.a.4 so that you can use it. a. finding the disk name: type sudo lsblk Then look for the drive name, e.g. /dev/sdb , that corresponds to the space that you called for. In most cases, it's /dev/sdb b. format the drive: type: VERY IMPORTANT-- PLEASE READ!!!! you only have to format a drive IF it is newly created. IF you are using a drive that was created from before, FORMATTING will delete everything you had from before. SO only do this once in the life of a disk! IF using an old disk, skip this step! sudo /home/taing/utils/formatDisk.sh [ drive part, e.g. 'sdb' from 2a ] e.g. sudo /home/tain/utils/formatDisk.sh sdb c. mount the drive: make a mount directory: EXAMPLE: sudo mkdir /mnt/ssd mount the drive: sudo /home/taing/utils/mountDrv.sh [ drive part, e.g. 'sdb' ] /mnt/ [ mount point from above ] Example: sudo /home/taing/utils/mountDrv.sh sdb /mnt/ssd This will mount /dev/sdb to /mnt/ssd d. create a directory you can use on /mnt/ssd : sudo mkdir /mnt/ssd/ [ username ] sudo chown [ username ] : [ username ] /mnt/ssd/ [ username ] EXAMPLE: sudo mkdir /mnt/ssd/aashna sudo chown aashna:aashna /mnt/ssd/aashna NOW you can read and write files to /mnt/ssd/aashan without being sudo e. REDIRECT /tmp NOTE: sometimes the /tmp directory can get full. To ensure this doesn't happen, I usually move /tmp off of the root partition cd /mnt/ssd #or where your newly created disk is sudo mkdir tmp sudo chmod a+w tmp sudo chmod a+r tmp soft-link cd / sudo mv tmp/ tmp.bak sudo ln -s /mnt/ssd/tmp Steps for WES setup \u00b6 f. run sentieon license: cd /home/taing/utils/ nohup ./sentieonLicense.sh & how to setup a wes run: (in the directory created in 2d.) \u00b6 change into your directory from 2d: cd /mnt/ssd/ [ username ] # clone the wes repository: git clone https://AashnaJhaveri@bitbucket.org/plumbers/cidc_wes.git # create a data directory: mkdir data # upload your fastqs into data copy out the config.yaml and metasheet.csv: cp cidc_wes/config.yaml . cp cidc_wes/metasheet.csv . Edit the config.yaml to fill the sentieon path which I believe is something: /home/taing/sentieon/sentieon.../bin fill the samples section- #Jingxin, ask aashna about this Edit metasheet.csv to define the Normal/Tumor pairs link the reference files: The reference files include, for example, the bwa index and the genome's FASTA file. ln -s /mnt/cidc_nfs/wes/ref_files Now you are ready to run RUN wes: \u00b6 source activate wes Your cmd line should be pre-pended with (wes) do a dry run to check for errors in config or metasheet snakemake -s cidc_wes/wes.snakefile -n If all is green you are good to go If there are errors, fix them FULL run: nohup time snakemake -s cidc_wes/wes.snakefile -j 64 > nohup.out & The 'nohup' allows you to log off. The 'time' will time the run for you. the -j 64 means to use 64. Use whatever number you want.","title":"Run WES on gcp"},{"location":"google-cloud/wes-gcp/#how-to-run-wes-pipeline-in-google-cloud","text":"Thanks Len and Aashna for sharing it.","title":"How to run WES pipeline in google cloud"},{"location":"google-cloud/wes-gcp/#how-to-get-your-google-cloud-credentials","text":"a. install the gcloud sdk: (it's on conda) conda install -c conda-forge google-cloud-sdk b. Authenticate: gcloud auth login NOTE: opens up a webbrowser, where I need to select the google account to allow access c. set project: gcloud config set project cidc-biofx","title":"how to get your google cloud credentials"},{"location":"google-cloud/wes-gcp/#create-the-new-instance","text":"a. Goto google cloud platform : Google Compute Engine -> click \"Create Instance\" give your instance type a unique name, e.g. wes-aashna-1 Machine type: select high-mem-64 (this has 64 cores) Boot disk: click \"Change\". click \"Custome Images\" (next to Application Images). select latest wes image-. AS OF 2019-02-04, it is wes-ver-1-1b Click \"Management, security, disks, networking, sole tenancy\". click Disks click \"Add new disk\" scroll down to size, and HERE you need to try to predict how much space you need to do your analysis e.g. 2T = 2048 scroll to the bottom, click \"Create\" NOW you will be brough back to the Google Compute Engine page click on your new instance when it is up and get the IP address login to your instance: gcloud compute ssh test-instance-1 WHERE test-instance-1 is the instance name. NOTE: this creates ~/.ssh/google_compute_engine and ~/.ssh/google_compute._engine.pub **NOTE: You only need to do this once. after ~/.ssh/google_compute_engine and ~/.ssh/google_compute._engine.pub is created, you can use this ssh cmd to log in instead: ssh -i [ your google-cloud-engine key ] [ your username ] @ [ ipaddress in 1b. ] e.g. ssh -i ~/.ssh/google_compute_engine aashna@XX.YY.ZZ.AA Please read Connecting to Linux instances and Connecting to instances using advanced methods for more details.","title":"CREATE the new instance:"},{"location":"google-cloud/wes-gcp/#formatting-and-mounting-the-second-drive-","text":"After logging in, the next step is to mount and format the drive that you apportioned in step 1.a.4 so that you can use it. a. finding the disk name: type sudo lsblk Then look for the drive name, e.g. /dev/sdb , that corresponds to the space that you called for. In most cases, it's /dev/sdb b. format the drive: type: VERY IMPORTANT-- PLEASE READ!!!! you only have to format a drive IF it is newly created. IF you are using a drive that was created from before, FORMATTING will delete everything you had from before. SO only do this once in the life of a disk! IF using an old disk, skip this step! sudo /home/taing/utils/formatDisk.sh [ drive part, e.g. 'sdb' from 2a ] e.g. sudo /home/tain/utils/formatDisk.sh sdb c. mount the drive: make a mount directory: EXAMPLE: sudo mkdir /mnt/ssd mount the drive: sudo /home/taing/utils/mountDrv.sh [ drive part, e.g. 'sdb' ] /mnt/ [ mount point from above ] Example: sudo /home/taing/utils/mountDrv.sh sdb /mnt/ssd This will mount /dev/sdb to /mnt/ssd d. create a directory you can use on /mnt/ssd : sudo mkdir /mnt/ssd/ [ username ] sudo chown [ username ] : [ username ] /mnt/ssd/ [ username ] EXAMPLE: sudo mkdir /mnt/ssd/aashna sudo chown aashna:aashna /mnt/ssd/aashna NOW you can read and write files to /mnt/ssd/aashan without being sudo e. REDIRECT /tmp NOTE: sometimes the /tmp directory can get full. To ensure this doesn't happen, I usually move /tmp off of the root partition cd /mnt/ssd #or where your newly created disk is sudo mkdir tmp sudo chmod a+w tmp sudo chmod a+r tmp soft-link cd / sudo mv tmp/ tmp.bak sudo ln -s /mnt/ssd/tmp","title":"formatting and mounting the second drive-"},{"location":"google-cloud/wes-gcp/#steps-for-wes-setup","text":"f. run sentieon license: cd /home/taing/utils/ nohup ./sentieonLicense.sh &","title":"Steps for WES setup"},{"location":"google-cloud/wes-gcp/#how-to-setup-a-wes-run-in-the-directory-created-in-2d","text":"change into your directory from 2d: cd /mnt/ssd/ [ username ] # clone the wes repository: git clone https://AashnaJhaveri@bitbucket.org/plumbers/cidc_wes.git # create a data directory: mkdir data # upload your fastqs into data copy out the config.yaml and metasheet.csv: cp cidc_wes/config.yaml . cp cidc_wes/metasheet.csv . Edit the config.yaml to fill the sentieon path which I believe is something: /home/taing/sentieon/sentieon.../bin fill the samples section- #Jingxin, ask aashna about this Edit metasheet.csv to define the Normal/Tumor pairs link the reference files: The reference files include, for example, the bwa index and the genome's FASTA file. ln -s /mnt/cidc_nfs/wes/ref_files Now you are ready to run","title":"how to setup a wes run: (in the directory created in 2d.)"},{"location":"google-cloud/wes-gcp/#run-wes","text":"source activate wes Your cmd line should be pre-pended with (wes) do a dry run to check for errors in config or metasheet snakemake -s cidc_wes/wes.snakefile -n If all is green you are good to go If there are errors, fix them FULL run: nohup time snakemake -s cidc_wes/wes.snakefile -j 64 > nohup.out & The 'nohup' allows you to log off. The 'time' will time the run for you. the -j 64 means to use 64. Use whatever number you want.","title":"RUN wes:"},{"location":"kraken/kraken_rstudio/","text":"How to Run Rstudio server on kraken \u00b6 install singularity and Rocker \u00b6 Also read Ming Tang's blog post https://divingintogeneticsandgenomics.rbind.io/post/run-rstudio-server-with-singularity-on-hpc/ You can not run any jobs on the login node, even conda install is not allowed. First, submit an interactive job to get a node: ssh kraken srun -t 1600 --mem = 60G -c 4 --pty bash From man srun : --time-min=<time> Set a minimum time limit on the job allocation. If specified, the job may have it's --time limit lowered to a value no lower than --time-min if doing so permits the job to begin execution ear\u2010 lier than otherwise possible. The job's time limit will not be changed after the job is allo\u2010 cated resources. This is performed by a backfill scheduling algorithm to allocate resources oth\u2010 erwise reserved for higher priority jobs. Acceptable time formats include \"minutes\", \"min\u2010 utes:seconds\", \"hours:minutes:seconds\", \"days-hours\", \"days-hours:minutes\" and \"days-hours:min\u2010 utes:seconds\". This option applies to job allocations. -c, --cpus-per-task=<ncpus> Request that ncpus be allocated per process. This may be useful if the job is multithreaded and requires more than one CPU per task for optimal performance. The default is one CPU per process. If -c is specified without -n, as many tasks will be allocated per node as possible while satis\u2010 fying the -c restriction. For instance on a cluster with 8 CPUs per node, a job request for 4 nodes and 3 CPUs per task may be allocated 3 or 6 CPUs per node (1 or 2 tasks per node) depending upon resource consumption by other jobs. Such a job may be unable to execute more than a total of 4 tasks. This option may also be useful to spawn tasks without allocating resources to the job step from the job's allocation when running multiple job steps with the --exclusive option. -N, --nodes=<minnodes[-maxnodes]> Request that a minimum of minnodes nodes be allocated to this job. A maximum node count may also be specified with maxnodes. If only one number is specified, this is used as both the minimum and maximum node count. The partition's node limits supersede those of the job. If a job's node limits are outside of the range permitted for its associated partition, the job will be left in a PENDING state. This permits possible execution at a later time, when the partition limit is changed. If a job node limit exceeds the number of nodes configured in the partition, the job will be rejected. Note that the environment variable SLURM_JOB_NUM_NODES (and SLURM_NNODES for backwards compatibility) will be set to the count of nodes actually allocated to the job. See the ENVIRONMENT VARIABLES section for more information. If -N is not specified, the default behavior is to allocate enough nodes to satisfy the requirements of the -n and -c options. The job will be allocated as many nodes as possible within the range specified and without delaying the initi\u2010 ation of the job. If number of tasks is given and a number of requested nodes is also given the number of nodes used from that request will be reduced to match that of the number of tasks if the number of nodes in the request is greater than the number of tasks. The node count specifi\u2010 cation may include a numeric value followed by a suffix of \"k\" (multiplies numeric value by 1,024) or \"m\" (multiplies numeric value by 1,048,576). This option applies to job and step allo\u2010 cations. -n, --ntasks=<number> Specify the number of tasks to run. Request that srun allocate resources for ntasks tasks. The default is one task per node, but note that the --cpus-per-task option will change this default. This option applies to job and step allocations.} After you get a computing node, you will see the prompt becomes something like: (base) [mtang@node03 mtang]$ . Note, conda singularity has problems to run. Ask the admin to install singularity on the server instead. Luckily, it is on kraken already. module load singularity pull the rocker image. read https://www.rocker-project.org/use/singularity/ cd /liulab/mtang mkdir singularity_images ; cd !$ singularity pull --name rstudio.simg docker://rocker/tidyverse To enable password authentication, set the PASSWORD environment variable and add the --auth-none=0 --auth-pam-helper-path=pam-helper options: PASSWORD = 'xyz' singularity exec --bind = /liulab/mtang rstudio.simg rserver --auth-none = 0 --auth-pam-helper-path = pam-helper --www-address = 127 .0.0.1 This will run rserver in a Singularity container. The --www-address=127.0.0.1 option binds to localhost (the default is 0.0.0.0 , or all IP addresses on the host). listening on 127.0.0.1:8787 . From your local workstation (e.g., mac book), ssh tunneling to the compute node. on your local mac: # check which ports are open sudo lsof -i -P -n | grep TCP ssh -f -L 59083 :localhost:59083 ssh -L 59083 :localhost:8787 -N node03 Go to your mac local browser, open localhost:59083 , type your HPC username and xyz as the password in this dummy example. You should have a Rstudio server running! set up local library \u00b6 You will want to install libraries so that the singularity container can use. If you go to your browser and inside Rstudio: > .libPaths () [1] \"/usr/local/lib/R/site-library\" \"/usr/local/lib/R/library\" You will see the library is pointing to the ones inside the container. Set up a .Renviron file in your HPC login node: if [ ! -e ${ HOME } /.Renviron ] then printf '\\nNOTE: creating ~/.Renviron file\\n\\n' echo 'R_LIBS_USER=~/R/%p-library/%v' >> ${ HOME } /.Renviron fi Now, go to Rstudio in the browser ---> Session ---> Restart R . > .libPaths () [1] \"/homes6/mtang/R/x86_64-pc-linux-gnu-library/3.6\" \"/usr/local/lib/R/site-library\" [3] \" /usr/local/lib/R/library You now see that \"/homes6/mtang/R/x86_64-pc-linux-gnu-library/3.6\" is my local library. If I install packages inside Rstudio, it will be installed there. Note : The .rstudio in your home directory /homes6/mtang/.rstudio/sessions/active/session-2cf8e174/suspended-session-data can grow large. Remember to clean it up (delete) it regularly.","title":"Run Rstudio on Kraken"},{"location":"kraken/kraken_rstudio/#how-to-run-rstudio-server-on-kraken","text":"","title":"How to Run Rstudio server on kraken"},{"location":"kraken/kraken_rstudio/#install-singularity-and-rocker","text":"Also read Ming Tang's blog post https://divingintogeneticsandgenomics.rbind.io/post/run-rstudio-server-with-singularity-on-hpc/ You can not run any jobs on the login node, even conda install is not allowed. First, submit an interactive job to get a node: ssh kraken srun -t 1600 --mem = 60G -c 4 --pty bash From man srun : --time-min=<time> Set a minimum time limit on the job allocation. If specified, the job may have it's --time limit lowered to a value no lower than --time-min if doing so permits the job to begin execution ear\u2010 lier than otherwise possible. The job's time limit will not be changed after the job is allo\u2010 cated resources. This is performed by a backfill scheduling algorithm to allocate resources oth\u2010 erwise reserved for higher priority jobs. Acceptable time formats include \"minutes\", \"min\u2010 utes:seconds\", \"hours:minutes:seconds\", \"days-hours\", \"days-hours:minutes\" and \"days-hours:min\u2010 utes:seconds\". This option applies to job allocations. -c, --cpus-per-task=<ncpus> Request that ncpus be allocated per process. This may be useful if the job is multithreaded and requires more than one CPU per task for optimal performance. The default is one CPU per process. If -c is specified without -n, as many tasks will be allocated per node as possible while satis\u2010 fying the -c restriction. For instance on a cluster with 8 CPUs per node, a job request for 4 nodes and 3 CPUs per task may be allocated 3 or 6 CPUs per node (1 or 2 tasks per node) depending upon resource consumption by other jobs. Such a job may be unable to execute more than a total of 4 tasks. This option may also be useful to spawn tasks without allocating resources to the job step from the job's allocation when running multiple job steps with the --exclusive option. -N, --nodes=<minnodes[-maxnodes]> Request that a minimum of minnodes nodes be allocated to this job. A maximum node count may also be specified with maxnodes. If only one number is specified, this is used as both the minimum and maximum node count. The partition's node limits supersede those of the job. If a job's node limits are outside of the range permitted for its associated partition, the job will be left in a PENDING state. This permits possible execution at a later time, when the partition limit is changed. If a job node limit exceeds the number of nodes configured in the partition, the job will be rejected. Note that the environment variable SLURM_JOB_NUM_NODES (and SLURM_NNODES for backwards compatibility) will be set to the count of nodes actually allocated to the job. See the ENVIRONMENT VARIABLES section for more information. If -N is not specified, the default behavior is to allocate enough nodes to satisfy the requirements of the -n and -c options. The job will be allocated as many nodes as possible within the range specified and without delaying the initi\u2010 ation of the job. If number of tasks is given and a number of requested nodes is also given the number of nodes used from that request will be reduced to match that of the number of tasks if the number of nodes in the request is greater than the number of tasks. The node count specifi\u2010 cation may include a numeric value followed by a suffix of \"k\" (multiplies numeric value by 1,024) or \"m\" (multiplies numeric value by 1,048,576). This option applies to job and step allo\u2010 cations. -n, --ntasks=<number> Specify the number of tasks to run. Request that srun allocate resources for ntasks tasks. The default is one task per node, but note that the --cpus-per-task option will change this default. This option applies to job and step allocations.} After you get a computing node, you will see the prompt becomes something like: (base) [mtang@node03 mtang]$ . Note, conda singularity has problems to run. Ask the admin to install singularity on the server instead. Luckily, it is on kraken already. module load singularity pull the rocker image. read https://www.rocker-project.org/use/singularity/ cd /liulab/mtang mkdir singularity_images ; cd !$ singularity pull --name rstudio.simg docker://rocker/tidyverse To enable password authentication, set the PASSWORD environment variable and add the --auth-none=0 --auth-pam-helper-path=pam-helper options: PASSWORD = 'xyz' singularity exec --bind = /liulab/mtang rstudio.simg rserver --auth-none = 0 --auth-pam-helper-path = pam-helper --www-address = 127 .0.0.1 This will run rserver in a Singularity container. The --www-address=127.0.0.1 option binds to localhost (the default is 0.0.0.0 , or all IP addresses on the host). listening on 127.0.0.1:8787 . From your local workstation (e.g., mac book), ssh tunneling to the compute node. on your local mac: # check which ports are open sudo lsof -i -P -n | grep TCP ssh -f -L 59083 :localhost:59083 ssh -L 59083 :localhost:8787 -N node03 Go to your mac local browser, open localhost:59083 , type your HPC username and xyz as the password in this dummy example. You should have a Rstudio server running!","title":"install singularity and Rocker"},{"location":"kraken/kraken_rstudio/#set-up-local-library","text":"You will want to install libraries so that the singularity container can use. If you go to your browser and inside Rstudio: > .libPaths () [1] \"/usr/local/lib/R/site-library\" \"/usr/local/lib/R/library\" You will see the library is pointing to the ones inside the container. Set up a .Renviron file in your HPC login node: if [ ! -e ${ HOME } /.Renviron ] then printf '\\nNOTE: creating ~/.Renviron file\\n\\n' echo 'R_LIBS_USER=~/R/%p-library/%v' >> ${ HOME } /.Renviron fi Now, go to Rstudio in the browser ---> Session ---> Restart R . > .libPaths () [1] \"/homes6/mtang/R/x86_64-pc-linux-gnu-library/3.6\" \"/usr/local/lib/R/site-library\" [3] \" /usr/local/lib/R/library You now see that \"/homes6/mtang/R/x86_64-pc-linux-gnu-library/3.6\" is my local library. If I install packages inside Rstudio, it will be installed there. Note : The .rstudio in your home directory /homes6/mtang/.rstudio/sessions/active/session-2cf8e174/suspended-session-data can grow large. Remember to clean it up (delete) it regularly.","title":"set up local library"},{"location":"miscellaneous/inkscape/","text":"inkscape \u00b6 There are two types of images: bitmap image and vector image. See this to get the idea of differences between them: https://www.lifewire.com/vector-and-bitmap-images-1701238 Photoshop for bitmap images (e.g. gel pictures) Illustrator for vector images (e.g. plots from python/R in pdf format) I use free tools: * gimp for bigmap images. * inkscape for vector images. Tutorials \u00b6 tutorial for inkscape .","title":"Making figures"},{"location":"miscellaneous/inkscape/#inkscape","text":"There are two types of images: bitmap image and vector image. See this to get the idea of differences between them: https://www.lifewire.com/vector-and-bitmap-images-1701238 Photoshop for bitmap images (e.g. gel pictures) Illustrator for vector images (e.g. plots from python/R in pdf format) I use free tools: * gimp for bigmap images. * inkscape for vector images.","title":"inkscape"},{"location":"miscellaneous/inkscape/#tutorials","text":"tutorial for inkscape .","title":"Tutorials"},{"location":"setup/hpc-info/","text":"HPC information for Liu lab \u00b6 Kraken server \u00b6 Nikos George (nikos[at]ds.dfci.harvard.edu)is the head of our Department Computing. Contact Nikos to get the Kraken (dept server) account. documentation is at http://dscomputing.dfci.harvard.edu/index.php/kraken/ you can only access when you login the parterns' VPN (pvc.partners.org/saml). Nikos will ask for your ssh public key. Generate a key pair by $ ssh-keygen -b 2048 send ~/.ssh/id_rsa.pub to Nikos. Remember sending only out the public key, not the private key id_sra . After you\u2019ve added your public key to the remote host, try logging in a few times. You\u2019ll notice that you keep getting prompted for your SSH key\u2019s password. If you\u2019re scratching your head wondering how this saves time, there\u2019s one more trick to know: ssh-agent . The ssh-agent program runs in the background on your local machine,and manages your SSH key(s). ssh-agent allows you to use your keys without entering their passwords each time\u2014exactly what we want when we frequently connect to servers. SSH agent is usually already running on Unix-based systems, but if not, you can use eval ssh-agent to start it. Then, to tell ssh-agent about our key, we use ssh-add : From bioinformatics data skills . $ ssh-add You have write permission to /liulab which is a lab share and to /cluster/liulab which is on a fast (IO intensive) cluster filesystem, strictly to be used as scratch space. Please note that /cluster/liulab is only available to the work nodes, but you can point your TMP_DIR to it Home Directory Storage \u00b6 Each user is given a home directory (/homes/username) that is mounted on all cluster nodes. It has a size limit of 20Gb. Please use it for basic login scripts and simple submit jobs. It is backed up daily, and backups are kept for 3 months. Lab Storage \u00b6 Each user has write permissions to their appropriate lab share (/name-of-lab.) Lab shares are mounted on all cluster nodes and can also be mounted on desktops and laptops. Size limits depend on the particular lab, this is where you put your regular data and work files. It is backed up weekly, and backups are kept for 3 months High Performance Scratch Storage \u00b6 Every lab has access to our high performance scratch space (/cluster/name-of-lab) Each user can create their own folder. This filesystem is managed by the GPFS parallel filesystem and is apropriate for data intensive jobs. It is mounted on all work nodes, but not on the head nodes. It is considered as a temporary storage. Files are not backed up and if the storage fills up we may delete any files, so once your analysis has been completed please move your files to your lab share. Local tmp storage \u00b6 Every work node has a small storage partition (approximately 100Gb) that is suitable for temp files (/tmp). This partition is not backed up and files can be deleted at any time. It is best not to use it since it is specific to the node and not shared across nodes. If your application is contained within the same node you can point TMPDIR to it. Iris/Daisy server \u00b6 Contact Annie Ng (annie[at]ds.dfci.harvard.edu) to connect to the Iris / Daisy (LiuLab server, cistrome.org).","title":"set up HPC accounts"},{"location":"setup/hpc-info/#hpc-information-for-liu-lab","text":"","title":"HPC information for Liu lab"},{"location":"setup/hpc-info/#kraken-server","text":"Nikos George (nikos[at]ds.dfci.harvard.edu)is the head of our Department Computing. Contact Nikos to get the Kraken (dept server) account. documentation is at http://dscomputing.dfci.harvard.edu/index.php/kraken/ you can only access when you login the parterns' VPN (pvc.partners.org/saml). Nikos will ask for your ssh public key. Generate a key pair by $ ssh-keygen -b 2048 send ~/.ssh/id_rsa.pub to Nikos. Remember sending only out the public key, not the private key id_sra . After you\u2019ve added your public key to the remote host, try logging in a few times. You\u2019ll notice that you keep getting prompted for your SSH key\u2019s password. If you\u2019re scratching your head wondering how this saves time, there\u2019s one more trick to know: ssh-agent . The ssh-agent program runs in the background on your local machine,and manages your SSH key(s). ssh-agent allows you to use your keys without entering their passwords each time\u2014exactly what we want when we frequently connect to servers. SSH agent is usually already running on Unix-based systems, but if not, you can use eval ssh-agent to start it. Then, to tell ssh-agent about our key, we use ssh-add : From bioinformatics data skills . $ ssh-add You have write permission to /liulab which is a lab share and to /cluster/liulab which is on a fast (IO intensive) cluster filesystem, strictly to be used as scratch space. Please note that /cluster/liulab is only available to the work nodes, but you can point your TMP_DIR to it","title":"Kraken server"},{"location":"setup/hpc-info/#home-directory-storage","text":"Each user is given a home directory (/homes/username) that is mounted on all cluster nodes. It has a size limit of 20Gb. Please use it for basic login scripts and simple submit jobs. It is backed up daily, and backups are kept for 3 months.","title":"Home Directory Storage"},{"location":"setup/hpc-info/#lab-storage","text":"Each user has write permissions to their appropriate lab share (/name-of-lab.) Lab shares are mounted on all cluster nodes and can also be mounted on desktops and laptops. Size limits depend on the particular lab, this is where you put your regular data and work files. It is backed up weekly, and backups are kept for 3 months","title":"Lab Storage"},{"location":"setup/hpc-info/#high-performance-scratch-storage","text":"Every lab has access to our high performance scratch space (/cluster/name-of-lab) Each user can create their own folder. This filesystem is managed by the GPFS parallel filesystem and is apropriate for data intensive jobs. It is mounted on all work nodes, but not on the head nodes. It is considered as a temporary storage. Files are not backed up and if the storage fills up we may delete any files, so once your analysis has been completed please move your files to your lab share.","title":"High Performance Scratch Storage"},{"location":"setup/hpc-info/#local-tmp-storage","text":"Every work node has a small storage partition (approximately 100Gb) that is suitable for temp files (/tmp). This partition is not backed up and files can be deleted at any time. It is best not to use it since it is specific to the node and not shared across nodes. If your application is contained within the same node you can point TMPDIR to it.","title":"Local tmp storage"},{"location":"setup/hpc-info/#irisdaisy-server","text":"Contact Annie Ng (annie[at]ds.dfci.harvard.edu) to connect to the Iris / Daisy (LiuLab server, cistrome.org).","title":"Iris/Daisy server"},{"location":"setup/setup_macos/","text":"set up my new mac pro \u00b6 old post https://divingintogeneticsandgenomics.rbind.io/post/set-up-my-new-mac-laptop/ download iterm \u00b6 configure color iTerm \u2192 Preferences \u2192 Profiles \u2192 colors -> Color Presets \u2192 Tango Dark By default, word jumps (option + \u2192 or \u2190) and word deletions (option + backspace) do not work. To enable these, go to \"iTerm \u2192 Preferences \u2192 Profiles \u2192 Keys \u2192 Load Preset... \u2192 Natural Text Editing \u2192 Boom! Head explodes\" iterm2 tips \u00b6 press command + click the file inside the terminal to open it! install oh-my-zsh \u00b6 sh -c \" $( curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh ) \" zsh syntax highlighting \u00b6 https://github.com/zsh-users/zsh-syntax-highlighting/blob/master/INSTALL.md Oh-my-zsh : Clone this repository in oh-my-zsh's plugins directory: git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ ZSH_CUSTOM :- ~/.oh-my-zsh/custom } /plugins/zsh-syntax-highlighting Activate the plugin in ~/.zshrc: my .zshrc has a plugin git , add it after that plugins=(git zsh-syntax-highlighting) https://github.com/zsh-users/zsh-syntax-highlighting/issues/530 shell integration \u00b6 https://iterm2.com/documentation-shell-integration.html logout and then login view images inside terminal \u00b6 put imgcat to your ~/bin . and add export PATH=$PATH:~/bin to your .zshrc https://www.iterm2.com/documentation-images.html A note on login shell and interactive shell. https://codingbee.net/rhcsa/rhcsa-starting-a-login-shell-or-interactive-shell-using-the-switch-user-su-command install conda \u00b6 conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge install sublime, rmate \u00b6 install R and Rstudio \u00b6 install brew \u00b6 /bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh ) \" brew install ncdu","title":"set up MacOS"},{"location":"setup/setup_macos/#set-up-my-new-mac-pro","text":"old post https://divingintogeneticsandgenomics.rbind.io/post/set-up-my-new-mac-laptop/","title":"set up my new mac pro"},{"location":"setup/setup_macos/#download-iterm","text":"configure color iTerm \u2192 Preferences \u2192 Profiles \u2192 colors -> Color Presets \u2192 Tango Dark By default, word jumps (option + \u2192 or \u2190) and word deletions (option + backspace) do not work. To enable these, go to \"iTerm \u2192 Preferences \u2192 Profiles \u2192 Keys \u2192 Load Preset... \u2192 Natural Text Editing \u2192 Boom! Head explodes\"","title":"download iterm"},{"location":"setup/setup_macos/#iterm2-tips","text":"press command + click the file inside the terminal to open it!","title":"iterm2 tips"},{"location":"setup/setup_macos/#install-oh-my-zsh","text":"sh -c \" $( curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh ) \"","title":"install oh-my-zsh"},{"location":"setup/setup_macos/#zsh-syntax-highlighting","text":"https://github.com/zsh-users/zsh-syntax-highlighting/blob/master/INSTALL.md Oh-my-zsh : Clone this repository in oh-my-zsh's plugins directory: git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ ZSH_CUSTOM :- ~/.oh-my-zsh/custom } /plugins/zsh-syntax-highlighting Activate the plugin in ~/.zshrc: my .zshrc has a plugin git , add it after that plugins=(git zsh-syntax-highlighting) https://github.com/zsh-users/zsh-syntax-highlighting/issues/530","title":"zsh syntax highlighting"},{"location":"setup/setup_macos/#shell-integration","text":"https://iterm2.com/documentation-shell-integration.html logout and then login","title":"shell integration"},{"location":"setup/setup_macos/#view-images-inside-terminal","text":"put imgcat to your ~/bin . and add export PATH=$PATH:~/bin to your .zshrc https://www.iterm2.com/documentation-images.html A note on login shell and interactive shell. https://codingbee.net/rhcsa/rhcsa-starting-a-login-shell-or-interactive-shell-using-the-switch-user-su-command","title":"view images inside terminal"},{"location":"setup/setup_macos/#install-conda","text":"conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge","title":"install conda"},{"location":"setup/setup_macos/#install-sublime-rmate","text":"","title":"install sublime, rmate"},{"location":"setup/setup_macos/#install-r-and-rstudio","text":"","title":"install R and Rstudio"},{"location":"setup/setup_macos/#install-brew","text":"/bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh ) \" brew install ncdu","title":"install brew"}]}